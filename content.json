{"pages":[],"posts":[{"title":"임베딩기법의역사와종류","text":"1.3 임베딩 기법의 역사와 종류1.3.1 통계 기반에서 뉴럴 네트워크 기반으로초기 임베딩 기법은 대부분 말뭉칭의 통계량을 직접 활용했다. 대표적으로 잠재 의미 분석 ** Latent Semantic Analysis**이다. 잠재 의미 분석잠재 의미 분석이란 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 커다란 행렬 ** Matrix에 특이값 분해 ** Singular Value Decomposition등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법 단어-문서 행렬에 잠재 의미 분석을 적용했다고 가정하자. 그런데 단어-문서 행렬을 행의 개수가 매우 많다. 어휘 수는 대개 10~20만 개일 것이다. 행렬의 대부분 요소 값은 0이다. 문서 하나에 모든 어휘가 쓰이는 경우는 매우 드물다. 이렇게 대부분의 요소 값이 0인 행렬을 희소 행렬 ** sparse matrix**이라고 한다. 이런 희소 행렬을 다른 모델의 입력값으로 쓰게 되면 계산량도 메모리 소비량도 쓸데없이 커진다. 그래서 원래 행렬의 차원을 축소해 사용한다. 단어와 문서를 기준으로 줄인다. 잠재 의미 분석 행렬 수행 대상 행렬은 여러 종류가 될 수 있다. TF-IDF 행렬 단어-문맥 행렬 점별 상호 정보량 행렬 최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목받고 있다. 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나 문장 내 일부분에 구멍을 뚫어 놓고 해당 단어가 무엇일지 맞추는 과정에서 학습된다. 1.3.2 단어 수준에서 문장 수준으로2017년 이전의 임베딩 기법들은 대게 단어 수준 모델이었다. NPLM, Word2Vec, Glove, FastText, Swivel 등이 있다. 단어 수준 임베딩 기법의 단점은 동음이의어 ** homonym을 구분하기 어렵다. 단어 형태가 같다면 동일한 단어로 보고, 문맥 정보를 해당 단어 벡터에 전달하기 때문이다.다행히도 ELMoEmbeddings from Language Modles**가 발표된 후 문장 수준 임베딩 기법이 주목받았다. 1.3.3 룰 -&gt; 엔드투엔드 -&gt; 프리트레인/파인 튜닝 1990년 : 사람이 피처를 직접 뽑음 2000년 중반 : 딥러닝 모델 주목, 입출력의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 유도 2018년 : 말뭉치로 임베딩을 만듬, 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트함 다운스트림 태스크 ** Downstream task** 품사판별 ** Part-of Speech tagging** 개체명 인식 ** Named Entity Recognition** 의미역 분석 ** Semantic Role Labeling** ######예시 품사 판별 : 나는 네가 지난 여름에 한 [일]을 알고 있다. → 일: 명사(Noun) 문장 성분 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. → 네가 지난 여름에 한 일 : 명사구(Noun Phrase) 의존 관계 분석 : [자연어 처리는] 늘 그렇듯이 [재미있다]. → 자언어 처리는, 재미있다 : 주격명사구(Nsub) 의미역 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. → 네가 지난 여름에 한 일 : 피행위주역(Patient) 상호 참조 해결 : 나는 어제 [성빈이]를 만났다. [그]는 스웨터를 입고 있었다. → 그=성빈이 업스트림 태스크 ** Upstream task** 단어/문장 임베딩을 프리트레인하는 작업 1.3.4 임베딩의 종류와 성능임베딩 기법 행렬 분해 예측 토픽 기반 행렬 분해 기반 방법말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법분해한 이후에 둘 중 하나의 행렬만 쓰거나 둘을 add 하거나 concatenate 임베딩으로 사용 예측 기반 방법어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습 토픽 기반 방법주어진 문서에 잠재된 주제를 추론** inference**하는 방식으로 임베딩을 수행모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환 하기 때문에 가능 임베딩 성능 평가성능 측정 대상 다운스트림 태스크는 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등이다.안타깝겓게도 한국어는 공개된 데이터가 많지 않아 높은 품질 측정을 기대하긴 어렵다.","link":"/2020/03/30/%EC%9E%84%EB%B2%A0%EB%94%A9%EA%B8%B0%EB%B2%95%EC%9D%98%EC%97%AD%EC%82%AC%EC%99%80%EC%A2%85%EB%A5%98/"},{"title":"한국어임베딩-임베딩의역할","text":"1.2 임베딩의 역할임베딩은 다음 역할을 수행할 수 있다 단어/문장 간 관련도 계산 의미적/문법적 정보 함축 전이 학습 1.2.1 단어/문장 간 관련도 계산현업에서는 2013년 구글 연구 팀이 발표한 Word2Vec이라는 기법이 대표적이다.단어들을 벡터로 바꾸는 방법이다.한국어 위키백과, KorQuAD, 네이버 영화 리뷰 말뭉치 등은 은전한닢으로 형태소 분석을 한 뒤 100차원으로 학습한 Word2Vec 임베딩 중 희망 이라는 단어의 벡터는 다음과 같다 [-0.00209 -0.03918 0.02419 … 0.01715 -0.04975 0.009300] 위 수식의 숫자들은 모두 100개이다. 100차원으로 임베딩을 했기 때문이다.단어를 벡터로 임베딩하는 순간 단어 벡터들 사이의 유사도 similarity를 계산하는 일이 가능해진다. 각 쿼리 단어별로 벡터 간 유사도 측정 기법의 일종인 코사인 유사도 cosine similarity 기준 상위 4개 희망 절망 학교 학생 소망 체념 초등 대학생 희망찬 절망감 중학교 대학원생 꿈 상실감 야학교 교직원 열망 번민 중학 학부모 희망과 코사인 유사도가 가장 높은 것은 소망이다.자연어일 때는 불가능했던 코사인 유사도 계산이 임베딩 덕분에 가능해 졌다.다음은 Word2Vec 임베딩을 통해서 단어 쌍 간 코사인 유사도를 시각화 한 것이다. 검정색일 수록 코사인 유사도가 높다 입베딩을 수행하면 벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다 1.2.2 의미/문법 정보 함축입베딩은 벡터인 만큼 사칙연산이 가능하다.단어 벡터간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. 단어 유추 평가 word analogy test 단어1 - 단어2 + 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어4를 배열한다 단어1 단어2 단어3 단어4 아들 딸 소년 소녀 아들 딸 아빠 엄마 아들 딸 남성 여성 남동생 여동생 소년 소녀 남동생 여동생 아빠 엄마 1.2.3 전이학습임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰인다. 문서 분류를 위한 딥러닝 모델을 만든다.예컨데 품질 좋은 임베딩을 쓰면 문서 분류 정확도와 학습 속도가 올라간다. 이렇게 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 전이 학습 transfer learning 이라고 한다. 전이학습전이 학습 모델은 제로부터 시작하지 않는다. 대규모 말뭉치를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 있다.문장의 극성을 예측하는 모델 양방향 LSTM에 어텐션 메커니즘을 적용 bidirectional Long Short-Term Memory, Attention 이 딥러닝의 모델의 입력값은 FastText 임베딩(100차원)을 사용했다. FastText 임베딩은 Word2Vec의 개선된 버전이며 59만 건에 이르는 한국어 문서를 미리 학습한 모델학습 데이터는 다음과 같다 이 영화 꿀잼 + 긍정 positive 이 영화 노잼 + 부정 negative 전이 학습 모델은 문장을 입력받으면 해당 문장이 긍정인지 부정인지를 출력한다. 문장을 형태소 분석한 뒤 각각의 형태소에 해당하는 FastText 단어 임베딩이 모델의 입력값이 된다. 위의 그래프로 처음 부터 하는 것 보다 FastText 임베딩을 사용한 모델의 성능이 좋다. 즉, 임베딩의 품질이 좋으면 수행하려는 Task의 성능 역시 올라간다. 왜 임베딩이 중요한지 깨달았다","link":"/2020/03/29/%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%98%EC%97%AD%ED%95%A0/"},{"title":"한국어임베딩-임베딩이란","text":"한국어임베딩github한국어 임베딩 서적을 공부하며 정리하는 글이 되겠습니다. 1.1 임베딩이란?기계의 자연어 이해와 생성은 연산 Computation 과 처리 Processing의 영역이다 자연어처리분야에서의 임베딩이란 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터 vector로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 단어나 문장 각각을 벡테로 변환해 벡터공간 vector space으로 ‘끼워 넣는다embed 는 의미에서 임베딩이라고 지었다. 단어-문서 행렬 Term-Document Matrix 구분 메밀꽃 필 무렵 운수좋은 날 사랑 손님과 어머니 삼포 가는길 기차 0 2 10 7 막걸리 0 1 0 0 선술집 0 1 0 0 위 와 같은 빈도표를 단어-문서 행렬이라고 부른다.row는 단어, column은 문서(작품)에 대응한다.운수좋은 날의 문서의 임베딩은 [2,1,1]이다. 막걸리라는 단어의 임베딩은 [0,1,0,0] 이다.표를 보면 ‘사랑 손님과 어머니’, ‘삼포 가는 길’이 사용하는 단어 목록이 상대적으로 많이 겹침을 알 수 있다.일르 바탕으로 ‘사랑 손님과 어머니’는 ‘’삼포 가는 길’과 ‘기차’라는 소재를 공유한다는 점에서 비슷한 작품임을 추정할 수 있다.","link":"/2020/03/29/%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%B4%EB%9E%80/"},{"title":"환경소개","text":"1.4.1 환경소개 Ubuntu 16.04.5 Python 3.5.2 Tensorflow 1.12.0 도커 구성하기본인은 Synology NAS를 사용하고 있으므로 NAS에서 docker 환경을 구성하는 방법을 포스팅하려고 한다.혹 NAS가 없는 경우 책에는 AWS로 하는 방법을 소개했으니 참고하자.도커이미지 123$ uname -a #ubuntu 환경 확인$ pwd #위치 확인$ cd /home #home으로 이동 1234root@docker-NLG:/# apt upgrade root@docker-NLG:/# apt update root@docker-NLG:/# apt install python3.5.2root@docker-NLG:/# apt install python3-pip pip3 라고 command에 쳤을 때 뭐라뭐라 길게 나오면 성공!더 정확하게 확인하려면 12root@docker-NLG:/# pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) 123root@docker-NLG:/# pip3 install --upgrade piproot@docker-NLG:/# pip --version pip 20.0.2 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6) 도커 다운을 위한 패키지 설치안된다면 앞에 sudo를 붙여보시길.. 1234apt install apt-transport-https apt install ca-certificatesapt install curl apt install software-properties-common apt-transport-https : 패키지 관리자가 https를 통해 데이터 및 패키지에 접근할 수 있도록 한다.ca-certificates : ca-certificate는 certificate authority에서 발행되는 디지털 서명. SSL 인증서의 PEM 파일이 포함되어 있어 SSL 기반 앱이 SSroot@docker-NLG:/# pip3 –version L 연결이 되어있는지 확인할 수 있다.curl : 특정 웹사이트에서 데이터를 다운로드 받을 때 사용software-properties-common : *PPA를 추가하거나 제거할 때 사용한다. curl 명령어로 도커 다운받기 &amp;&amp; repository에 경로 추가하기1234root@docker-NLG:/# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - OK root@docker-NLG:/# add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&quot; curl 명령어의 옵션 f : HTTP 요청 헤더의 contentType을 multipart/form-data로 보낸다.s : 진행 과정이나 에러 정보를 보여주지 않는다.(–silent)S : SSL 인증과 관련있다고 들었는데, 정확히 아시는 분 있다면 댓글 부탁!L : 서버에서 301, 302 응답이 오면 redirection URL로 따라간다.apt-key : apt가 패키지를 인증할 때 사용하는 키 리스트를 관리한다. 이 키를 사용해 인증된 패키지는 신뢰할 수 있는 것으로 간주한다. add 명령어는 키 리스트에 새로운 키를 추가하겠다는 의미이다. add-apt-repository : PPA 저장소를 추가해준다. apt 리스트에 패키지를 다운로드 받을 수 있는 경로가 추가된다. docker 패키지가 검색되는지 확인1234root@docker-NLG:/# apt-get update root@docker-NLG:/# apt-cache search docker-ce docker-ce-cli - Docker CLI: the open-source application container engine docker-ce - Docker: the open-source application container engine apt update : 저장소의 패키지 갱신 도커 설치하기 &amp;&amp; ubuntu를 도커그룹으로 입력123root@docker-NLG:/# apt-get install docker-ceroot@docker-NLG:/# usermod -aG docker $USER Usage: usermod [options] LOGIN nvidia-docker 설치위의 단계까지 끝내면 일반적인 도커 기능들을 이용하실 수 있습니다. 하지만 NVIDIA의 GPU를 이용하시면서 여러 환경의 CUDA Tookit을 이용하실 경우 nvidia-docker라는 확장 기능을 추가하시면 보다 편리하게 사용하실 수 있습니다. nvidia-docker를 설치하고자 하실 경우 호스트 운영체제에 먼저 NVIDIA 드라이버가 설치되어 있어야 합니다. NVIDIA의 그래픽카드 또는 GPU를 사용하지 않는 경우 이 과정을 진행하고 도커 설치과정을 끝내실 수 있습니다. 우분투에서 NVIDIA 드라이버 설치 방법12345678910root@docker-NLG:/# curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey|apt-key add - OKroot@docker-NLG:/# distribution=$(. /etc/os-release;echo $ID$VERSION_ID) root@docker-NLG:/# curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list|tee /etc/apt/sources.list.d/nvidia-docker.list deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /root@docker-NLG:/# apt-get update nvidia-docker 실행우분투에서 nvidia-driver받기 1234root@docker-NLG:/# apt-get install -y nvidia-container-toolkitroot@docker-NLG:/# apt-get install systemdroot@docker-NLG:/# systemctl restart dockerroot@docker-NLG:/# docker run --gpus all nvidia/cuda:9.0-base nvidia-smi 도커실행하기1root@docker-NLG:/home# git clone https://github.com.rastgo/embedding 참고 링크 오픈소스링크TensorFlow : https://www.tensorflow.orgGensim : https://radimrehurek.com/gensimFastText : https://fasttext.ccGloVe : https://nlp.stanford.edu/projects/gloveSwivel : https://github.com/tensorflow/models/tree/master/research/swivelELMo : https://allennlp.org/elmoBERT : https://github.com/google-research/bertScikit-Learn : https://scikit-learn.orgKoNLPy : http://konlpy.org/en/latest/Mecab : http://eunjeon.blogspot.com/soynlp : https://github.com/lovit/soynlpKhaiii : https://github.com/kakao/khaiii https://tech.kakao.com/2018/12/13/khaiii/Bokeh : https://docs.bokeh.org/sentencepiece : https://github.com/google/sentencepiece","link":"/2020/03/30/%ED%99%98%EA%B2%BD%EC%86%8C%EA%B0%9C/"}],"tags":[{"name":"한국어임베딩","slug":"한국어임베딩","link":"/tags/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/"},{"name":"embedding","slug":"embedding","link":"/tags/embedding/"}],"categories":[{"name":"자연어처리","slug":"자연어처리","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"임베딩","slug":"자연어처리/임베딩","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/%EC%9E%84%EB%B2%A0%EB%94%A9/"}]}