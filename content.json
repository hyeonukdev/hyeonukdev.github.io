{"pages":[],"posts":[{"title":"클래스란 무엇인가","text":"Class가 무엇인가?# 자동차 1 car_brand1 = &quot;Kia&quot; car_color1 = &quot;Red&quot; car_year1 = 1996 # 자동차 2 car_brand2 = &quot;Hyundai&quot; car_color3 = &quot;Blue&quot; car_year3 = 2009 # 자동차 3 car_brand3 = &quot;BMW&quot; car_color3 = &quot;Black&quot; car_year3 = 2000브랜드, 색상, 제작년도가 있는 코드 만약 100대, 1000대 이상이 되면 일일이 쓰면 비효율적!→ 클래스를 사용하자 class Car: def __init__(self, brand, color, year): self.brand = brand self.color = color self.year = year car1 = Car(&quot;Kia&quot;, &quot;Red&quot;, 1996) car2 = Car(&quot;Hyundai&quot;, &quot;Blue&quot;, 2006) car3 = Car(&quot;BMW&quot;, &quot;Black&quot;, 2000)Car 는 자동차 클래스car1, car2, car3 객체는 Car 클래스의 ‘인스턴스’ ==클래스 : 비슷한 속성을 가진 객체를 묶는 큰 틀== 클래스 변수class Car: honk = &quot;빵빵&quot; print honk # NameError: name &apos;honk&apos; is not definedhonk이라는 변수는 외부에서 호출이 안된다 클래스 안에 있는 변수는 ‘해당 클래스의 인스턴트’를 통해서 호출 my_car = Car() my_car.honk # &apos;빵빵&apos; Car().honk self는 뭔가요?클래스 메소드의 첫 번째 인수로 self를 써 줘야지해당 메소드를 인스턴스의 메소드로 사용 가능쉽게 : 이 메소드를 부르는 객체가 해당 클래스의 인스턴스 인지 확인하려고 class Car: honk = &quot;빵빵&quot; def set_info(self, color, year): self.color = color self.year = year def get_info(self): print(&quot;color : %s , year: %d&quot; %(self.color, self.year)) my_car1 = Car() my_car1.set_info(&quot;Red&quot;, 2017) my_car1.get_info() #=&gt; color : Red ,year: 2017 init 이란?class Car: honk = &quot;빵빵&quot; def set_info(self, color, year): self.color = color self.year = year def get_info(self): print &quot;color : %s ,year: %d&quot; % (self.color, self.year) my_car = Car() my_car.set_info(&quot;Red&quot;, 2017) my_car.get_info() #=&gt; color:Red, year:2017 new_car = Car()0 new_car.get_info() # AttributeError: Car instance has no attribute &apos;color&apos;오류발생! set_info()를 안하고 get_info()를 먼저 불렀다…클래스의 인스턴트를 생성할 때, 오류를 줄이고 싶다! init 을 사용하자초기화 메소드, 생성자라고 불림인스턴스 = 클래스(변수1,변수2..) class Car: honk = &quot;빵빵&quot; def __init__(self, color, year): self.color = color self.year = year print &quot;새로운 Car 인스턴스가 생성되었습니다.&quot; def get_info(self): print &quot;color : %s ,year: %d&quot; % (self.color, self.year) my_car = Car(&quot;Red&quot;, 2017) # 새로운 Car 인스턴스가 생성되었습니다. my_Car.get_info() # color : Red, year: 2017새로운 인스턴스를 만들고 난 후 “새로운 Car 인스턴스가 생성되었습니다” 출력 즉 , init 메소드가 호출 되었다. 클래스 상속? 상속 : 물려받는다? 유산을 상속하다 부모의 클래스가 존재하고 그 부모 클래스를 ‘상속’받은 자식 클래스를 만들 수 있다.자식클래스는 부모가 가진 메소드나 변수를 물려받아 그대로 사용 가능! class Person: def __init__(self, name, age): self.name = name self.age = age def info(self): print(&quot;이름: %s, 나이: %d&quot; %(self.name, self.age)) class Employee(Person): pass em = Employee(&quot;goorm&quot;,20) em.info() #=&gt; 이름: goorm, 나이: 20Person 클래스는 init 메소드를 통해 이름, 나이를 초기값으로 받는다 info메소드를 통해 이름과 나이를 출력 Employee클래스를 보면 Person을 상속받았다 그래서 info를 동일하게 쓸 수 있다. 만약 메소드를 변형한다면?메소드 오버라이딩이라고 부른다 class Person: def __init__(self, name, age): self.name =name self.age = age def info(self): print &quot;나는 부모 클래스 입니다.&quot; class Employee(Person): def info(self): print &quot;나는 자식 클래스 입니다.&quot; per = Person(&quot;Python&quot;, 100) per.info() #=&gt; &quot;나는 부모 클래스 입니다.&quot; em = Employee(&quot;goorm&quot;,20) em.info() #=&gt; &quot;나는 자식 클래스 입니다.&quot;원래는 “나는 부모 클래스 입니다” 이지만 Person을 상속받아 자식클래스, 즉, Employee에서 메소드를 변형, 즉, 오버라이딩했다 따라서 “나는 자식 클래스 입니다”가 출력 예시를 하나 만들어보자123class 클래스명(상속클래스): def __init__(self, 매개변수): 인스턴스 속성 123456class Klass(object): def __init__(self, name): self.name = name def getName(self): return self.name __init__은 객체 생성 후 초기화 함수이다.객체가 생성될 때 내부의 이름공간에 속성을 할당하여 초기화 역할만 한다. 이제 객체를 생성해 보자 1k = Klass(\"객체 생성\") 이름을 조회해 보자 1k.name 1결과 '객체 생성' k.name 과 k.getName()은 동일하다 Python의 가장 큰 특징이다. 굳이 getName을 사용하지 않아도 name로만 조회가 가능하다. 객체와 인스턴스의 관계클래스는 객체를 만드는 도구이다.클래스로 객체를 생성하고,객체를 사용해서 특정기능을 처리할 수 있다. 여러개의 객체를 생성해 보자 123k1 = Klass(\"객체 생성1\")k2 = Klass(\"객체 생성2\")k3 = Klass(\"객체 생성3\") 생성관계isinstance : 클래스와 객체의 생성관계를 확인하는 내장함수 1isinstance(k, Klass) 1결과 : True 1isinstance(k1, Klass) 1결과 : True 즉 k와 k1의 Klass를 통해서 생성된 것이다. 함수와 메소드 구분하기함수들은 객체가 생성된 다음에 호출하기 때문에 메소드라고한다.예를 들면 getName()을 쓰려면 Klass에서 만든 인스턴스를 이용해서 호출할 때 사용하는 것이다. 클래스에 어떤 메소드가 있는지 확인해보자 1Klass.__dict__ 1k.__init__ 객체를 호출할 때는 함수가 아니라 메소드로 처리함을 알 수 있다.self에서는 객체에서 자동으로 전달되고 두 번째 인자에만 해당하는 값을 문자열로 전달하면 된다. 12k.__init__(\"메소드로 갱신\")k.name 1결과 : '메소드로 갱신'","link":"/2020/04/01/Python/%ED%81%B4%EB%9E%98%EC%8A%A4%EB%9E%80%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80/"},{"title":"논문","text":"논문의 구조 논문은 표현의 간결성(conciseness)과 명료성(clarity)이 매우 중요 ​ -영어 논문의 구조 나는 이런 문제를 풀거야 (abstract) 사실 이 문제는 이런 동기에서 연구가 시작된건데 (introduction) 관련해서 이런저런 접근들이 있었지 (related works) 난 이런 새로운 방식으로 접근해보려고 하는데 (method) 정말 이게 잘 먹히는지 실험도 해봤어 (experiment) 이를 통해 이런 사실도 알아냈지만 한계점도 있지 (discussion) 마지막으로 귀찮은 너를 위해 요약 (conclusion) 논문의 핵심은 ‘내가 주어진 문제에서 이러한 기여(contribution)를 했다’ ​ 논문 고르기 구글스칼라 이용 관심있는 키워드를 넣고 논문을 검색 그 분야의 개략적인 연구들을 훑어보려면 관련 키워드와 함께 ‘review’, ‘survey’, ‘tutorial’ 등을 넣고 검색 이들은 특정 문제를 푸는 일반 논문들과 달리, 관련 연구들을 종합하거나 (review), 조사하거나 (survey), 쉽게 설명하고 있다. (tutorial) Abstract 초록읽기 하이라이트 같은 거 개요 -&gt; In this paper -&gt; 방법 -&gt; 결과 순으로 적힘 이 논문이 ‘무슨 문제’를 풀려고 했고, ‘어떠한 새로운 기여’를 담고 있는지 파악 ​ Conclusion 결론 읽기 내가 제대로 이해했는지 확인하기 위해 결론 먼저 읽음 논문이 무슨 문제를 풀려했고, 어떠한 기여를 했는지 아는게 중요 ​ Instroduction 서론 읽기 본론은 지엽적인 문제해결만을 다룸 서론에서는 주요 연구들을 한줄 요약들과 함께 친절히 소개해줌 소개되는 논문들은 꼭 읽어야하는 논문 위주 (1) 내가 어떤 문제를 풀고 있는지 (2) 관련 연구들은 이 문제를 어떻게 풀어왔는지 (3) 마지막으로 나는 그들과 달리 어떤 입장에서 문제를 해결했는지 ​ 표/그림 보기영어 독해를 쉽게하는 방법 중 하나는 ‘앞에 나올 내용을 예상하며 읽는 것’이다. 이제까지 초록, 결론, 서론을 읽었던 것은 모두 본론에 어떤 내용이 나올지 잘 예측할 수 있기 위해서였다. 여기에 또 한가지 본문 이해에 도움을 주는 소재가 있다면 바로 표와 그림들 ​ Methods &amp; Experiments 이전까지는 무엇을 왜 에 대한 내용 방법 및 실험은 어떻게 에 대한 본연구의 자세한 설명 수식의 역할만 이해한다면 디테일은 일단 패스 중요한건 그 수식이 인풋으로 무엇을 받아 아웃풋으로 무엇을 내놓는지 이해하는 것이다. 그리고 왜 이 수식이 필요한지, 없으면 어떤 일이 벌어지는지를 이해하는 것 역시 중요하다. ​ 중요한건 수식이 아니라 ‘내가 뭘 읽고 있는지’와 ‘내가 왜 읽고 있는지’의 능동적 이해 자세이다. 혼미해지는 정신 꽉 부여잡고 이 논문의 핵심스토리에 집중하자. ​ 출처 http://gradschoolstory.net/terry/readingpapers/ 영어 못해도 논문 잘 읽는 법‘그 발번역 정말 못읽겠더라. 차라리 원서 읽어.’ ‘맞아맞아~ 어떻게 한글이 영어보다 어렵니? 원서가 훨씬 쉬운 듯’ 대학생 초년 시절, 영어가 너무 벅찬던 내가 운좋게 번역본이라도 구해 들고 있을지면 친구들은 항상 내게 이런 말을 건냈다. 번역본이 훨씬 어렵지 않냐면서 말이다. ‘당연하지&amp;#82… gradschoolstory.net ​","link":"/2020/04/01/%EB%85%BC%EB%AC%B8/%EB%85%BC%EB%AC%B8/"},{"title":"1장정리","text":"1.6 1장의 요약 임베딩이란 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리킴 임베딩을 사용하면 단어/문장 간 관련도를 계산할 수 있음 임베딩에는 믜미적/문법적 정보가 함축돼 있음 임베딩은 다른 딥러닝 모델의 입력값으로 쓰일 수 있음 임베딩 기법은 (1) 통계 기반에서 뉴럴 네트워크 기반으로 (2) 단어 수준에서 문장 수준으로 (3) 엔드투엔드에서 프리트레인/파인 튜닝 방식으로 발전해옴 임베딩 기법은 크게 행렬 분해 모델, 에측 기반 방법, 토픽 기반 기법 등으로 나눠짐 이 책이 다루는 데이터의 최소 단위는 토큰임. 문장은 토큰의 집합, 문서는 문장의 집합, 말뭉치는 문서의 집합을 가리킴. 말뭉치 &gt; 문서 &gt; 문장 &gt; 토큰. 어휘 집합은 말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토크나이즈를 실시한 후 중복을 제거한 토큰들의 집합임","link":"/2020/03/31/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/1%EC%9E%A5%EC%A0%95%EB%A6%AC/"},{"title":"데이터와주요용어","text":"1.5 이 책이 다루는 데이터와 주요 용어 텍스트(text) 말뭉치(corpus) : 텍스트 데이터 표본(Sample) : 특정한 목적을 가지고 수집한 말뭉치 컬렉션(collection) : 말뭉치에 속한 각각의 집합 문장(sentence) : 이 책에서 다루는 데이터의 기본 단위, 마침표나 느낌표, 물음표와 같은 기호로 구분된 문자열 문서(document), 단락(paragraph)의 집합 : 생각이나, 감정, 정보를 공유하는 문장의 집합, 줄바꿈(\\n) 문자로 구분된 문자열 토큰(token), 단어(word), 형태소(morpheme), 서브워드(subword) : 이 책에서 다루는 가장 작은 단위 토크나이즈(tokenize) : 문장을 토큰 시쿼스로 분석하는 과정, 토큰 시퀀스는 문장을 토큰으로 나누는 것, 토큰 구분자는 쉼표를 주로 사용 어휘 집합(vocabulary) : 말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토크나이즈를 실시한 후 중복을 제거한 토큰들의 집합 미등록 단어(unknown word) : 어휘 집합에 없는 토큰","link":"/2020/03/31/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80%EC%A3%BC%EC%9A%94%EC%9A%A9%EC%96%B4/"},{"title":"어떤단어가많이쓰였는가","text":"2.2 어떤 단어가 많이 쓰였는가2.2.1 백오브워즈 가정수학에서 백이란 중복 원소를 허용한 집합 multiset을 뜻한다. 원소의 순서는 고려하지 않는다. 어쩌면 중복 집합과 같다.자연어 처리 분야에서는 백오브워즈 bag of words란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것 저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있을 것 주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 역시 비슷하 것 빈도를 그대로 백오브워즈로 쓴다면 많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 것 위 처럼 문장을 단어로 쪼개고 임의의 주머니에 넣고 뽑았을 때 등장하면 1 아니면 0을 반영한 것이다. 백오브워즈 임베딩은 단순하지만 정보 검색 ** Information Retrieval분야에서 많이 쓰인다.사용자의 질의 ** query에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈 임베딩으로 변환하고 질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 유사도가 가장 높은 문서를 사용자에게 노출한다. 2.2.2 TF-IDF단어 빈도 또는 등장 여부를 그대로 임베딩으로 쓰는 것에는 단점이 있다. 해당 단어가 많이 나왔다고 하더라도 문서의 주제를 가늠하기 어렵다. 이유는 다음과 같다. ‘을/를’, ‘이/가’ 같은 조사들이 한국어 문서에 등장한다. 이 것으로 문서의 주제를 추측하기 어렵다.이런 단점을 보완하기 위해서 Term Frequency-Inverse Document Frequency이다.단어-문서 행렬에 가중치를 계산해 행렬 원소를 바꾼다. 이 또한 단어 등장 순서는 고려하지 않는다. 위 수식을 자세히 알아보자 TF는 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지 빈도를 나타낸다. A라는 단어가 문서1에 10번, 문서2에 5번 쓰였다면 문서1 - 단어A의 TF는 10, 문서2 - 단어A의 TF는 5 DF란 특정 단어가 나타난 문서의 수를 뜻한다. 문서1과 문서2에만 A가 등장했으므로 DF는 2D DF가 클수록 다수 문서에 쓰이는 범용적 단어이다 IDF는 전체 문서 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값이다. 그 값이 클수록 특이한 단어이다. 주제 예측 능력과 직결된다 결국 TF-IDF는 어떤 단어의 주제 예측 능력이 강할 수록 가중치가 커지고 그 반대의 경우 작아진다어떤 단어의 TF가 높으면 TF-IDF 값 역시 커진다단어 사용 빈도는 저자가 상정한 주제와 관련을 맺고 있을 거라는 가정에 기초한 것이다 2.2.3 Deep Averaging NetworkDeep Averaging Network는 백오브워즈 가정의 뉴럴 네트워크 버전이다. 예를 들어 애비는 종이었다 라는 문장이 있다면{애비, 종, 이, 었, 다}에 속한 단어의 임베딩을 평균을 취해 만든다. 문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지 그 빈도만을 고려한다. 문장 임베딩을 입력받아 해당 문서가 어떤 범주인지 분류 classifiation 한다.","link":"/2020/03/31/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%96%B4%EB%96%A4%EB%8B%A8%EC%96%B4%EA%B0%80%EB%A7%8E%EC%9D%B4%EC%93%B0%EC%98%80%EB%8A%94%EA%B0%80/"},{"title":"임베딩기법의역사와종류","text":"1.3 임베딩 기법의 역사와 종류1.3.1 통계 기반에서 뉴럴 네트워크 기반으로초기 임베딩 기법은 대부분 말뭉칭의 통계량을 직접 활용했다. 대표적으로 잠재 의미 분석 ** Latent Semantic Analysis**이다. 잠재 의미 분석잠재 의미 분석이란 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 커다란 행렬 ** Matrix에 특이값 분해 ** Singular Value Decomposition등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법 단어-문서 행렬에 잠재 의미 분석을 적용했다고 가정하자. 그런데 단어-문서 행렬을 행의 개수가 매우 많다. 어휘 수는 대개 10~20만 개일 것이다. 행렬의 대부분 요소 값은 0이다. 문서 하나에 모든 어휘가 쓰이는 경우는 매우 드물다. 이렇게 대부분의 요소 값이 0인 행렬을 희소 행렬 ** sparse matrix**이라고 한다. 이런 희소 행렬을 다른 모델의 입력값으로 쓰게 되면 계산량도 메모리 소비량도 쓸데없이 커진다. 그래서 원래 행렬의 차원을 축소해 사용한다. 단어와 문서를 기준으로 줄인다. 잠재 의미 분석 행렬 수행 대상 행렬은 여러 종류가 될 수 있다. TF-IDF 행렬 단어-문맥 행렬 점별 상호 정보량 행렬 최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목받고 있다. 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나 문장 내 일부분에 구멍을 뚫어 놓고 해당 단어가 무엇일지 맞추는 과정에서 학습된다. 1.3.2 단어 수준에서 문장 수준으로2017년 이전의 임베딩 기법들은 대게 단어 수준 모델이었다. NPLM, Word2Vec, Glove, FastText, Swivel 등이 있다. 단어 수준 임베딩 기법의 단점은 동음이의어 ** homonym을 구분하기 어렵다. 단어 형태가 같다면 동일한 단어로 보고, 문맥 정보를 해당 단어 벡터에 전달하기 때문이다.다행히도 ELMoEmbeddings from Language Modles**가 발표된 후 문장 수준 임베딩 기법이 주목받았다. 1.3.3 룰 -&gt; 엔드투엔드 -&gt; 프리트레인/파인 튜닝 1990년 : 사람이 피처를 직접 뽑음 2000년 중반 : 딥러닝 모델 주목, 입출력의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 유도 2018년 : 말뭉치로 임베딩을 만듬, 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트함 다운스트림 태스크 ** Downstream task** 품사판별 ** Part-of Speech tagging** 개체명 인식 ** Named Entity Recognition** 의미역 분석 ** Semantic Role Labeling** ######예시 품사 판별 : 나는 네가 지난 여름에 한 [일]을 알고 있다. → 일: 명사(Noun) 문장 성분 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. → 네가 지난 여름에 한 일 : 명사구(Noun Phrase) 의존 관계 분석 : [자연어 처리는] 늘 그렇듯이 [재미있다]. → 자언어 처리는, 재미있다 : 주격명사구(Nsub) 의미역 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. → 네가 지난 여름에 한 일 : 피행위주역(Patient) 상호 참조 해결 : 나는 어제 [성빈이]를 만났다. [그]는 스웨터를 입고 있었다. → 그=성빈이 업스트림 태스크 ** Upstream task** 단어/문장 임베딩을 프리트레인하는 작업 1.3.4 임베딩의 종류와 성능임베딩 기법 행렬 분해 예측 토픽 기반 행렬 분해 기반 방법말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법분해한 이후에 둘 중 하나의 행렬만 쓰거나 둘을 add 하거나 concatenate 임베딩으로 사용 예측 기반 방법어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습 토픽 기반 방법주어진 문서에 잠재된 주제를 추론** inference**하는 방식으로 임베딩을 수행모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환 하기 때문에 가능 임베딩 성능 평가성능 측정 대상 다운스트림 태스크는 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등이다.안타깝겓게도 한국어는 공개된 데이터가 많지 않아 높은 품질 측정을 기대하긴 어렵다.","link":"/2020/03/30/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%9E%84%EB%B2%A0%EB%94%A9%EA%B8%B0%EB%B2%95%EC%9D%98%EC%97%AD%EC%82%AC%EC%99%80%EC%A2%85%EB%A5%98/"},{"title":"한국어임베딩-임베딩이란","text":"한국어임베딩github한국어 임베딩 서적을 공부하며 정리하는 글이 되겠습니다. 1.1 임베딩이란?기계의 자연어 이해와 생성은 연산 Computation 과 처리 Processing의 영역이다 자연어처리분야에서의 임베딩이란 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터 vector로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 단어나 문장 각각을 벡테로 변환해 벡터공간 vector space으로 ‘끼워 넣는다embed 는 의미에서 임베딩이라고 지었다. 단어-문서 행렬 Term-Document Matrix 구분 메밀꽃 필 무렵 운수좋은 날 사랑 손님과 어머니 삼포 가는길 기차 0 2 10 7 막걸리 0 1 0 0 선술집 0 1 0 0 위 와 같은 빈도표를 단어-문서 행렬이라고 부른다.row는 단어, column은 문서(작품)에 대응한다.운수좋은 날의 문서의 임베딩은 [2,1,1]이다. 막걸리라는 단어의 임베딩은 [0,1,0,0] 이다.표를 보면 ‘사랑 손님과 어머니’, ‘삼포 가는 길’이 사용하는 단어 목록이 상대적으로 많이 겹침을 알 수 있다.일르 바탕으로 ‘사랑 손님과 어머니’는 ‘’삼포 가는 길’과 ‘기차’라는 소재를 공유한다는 점에서 비슷한 작품임을 추정할 수 있다.","link":"/2020/03/29/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%B4%EB%9E%80/"},{"title":"한국어임베딩-임베딩의역할","text":"1.2 임베딩의 역할임베딩은 다음 역할을 수행할 수 있다 단어/문장 간 관련도 계산 의미적/문법적 정보 함축 전이 학습 1.2.1 단어/문장 간 관련도 계산현업에서는 2013년 구글 연구 팀이 발표한 Word2Vec이라는 기법이 대표적이다.단어들을 벡터로 바꾸는 방법이다.한국어 위키백과, KorQuAD, 네이버 영화 리뷰 말뭉치 등은 은전한닢으로 형태소 분석을 한 뒤 100차원으로 학습한 Word2Vec 임베딩 중 희망 이라는 단어의 벡터는 다음과 같다 [-0.00209 -0.03918 0.02419 … 0.01715 -0.04975 0.009300] 위 수식의 숫자들은 모두 100개이다. 100차원으로 임베딩을 했기 때문이다.단어를 벡터로 임베딩하는 순간 단어 벡터들 사이의 유사도 similarity를 계산하는 일이 가능해진다. 각 쿼리 단어별로 벡터 간 유사도 측정 기법의 일종인 코사인 유사도 cosine similarity 기준 상위 4개 희망 절망 학교 학생 소망 체념 초등 대학생 희망찬 절망감 중학교 대학원생 꿈 상실감 야학교 교직원 열망 번민 중학 학부모 희망과 코사인 유사도가 가장 높은 것은 소망이다.자연어일 때는 불가능했던 코사인 유사도 계산이 임베딩 덕분에 가능해 졌다.다음은 Word2Vec 임베딩을 통해서 단어 쌍 간 코사인 유사도를 시각화 한 것이다. 검정색일 수록 코사인 유사도가 높다 입베딩을 수행하면 벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다 1.2.2 의미/문법 정보 함축입베딩은 벡터인 만큼 사칙연산이 가능하다.단어 벡터간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. 단어 유추 평가 word analogy test 단어1 - 단어2 + 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어4를 배열한다 단어1 단어2 단어3 단어4 아들 딸 소년 소녀 아들 딸 아빠 엄마 아들 딸 남성 여성 남동생 여동생 소년 소녀 남동생 여동생 아빠 엄마 1.2.3 전이학습임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰인다. 문서 분류를 위한 딥러닝 모델을 만든다.예컨데 품질 좋은 임베딩을 쓰면 문서 분류 정확도와 학습 속도가 올라간다. 이렇게 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 전이 학습 transfer learning 이라고 한다. 전이학습전이 학습 모델은 제로부터 시작하지 않는다. 대규모 말뭉치를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 있다.문장의 극성을 예측하는 모델 양방향 LSTM에 어텐션 메커니즘을 적용 bidirectional Long Short-Term Memory, Attention 이 딥러닝의 모델의 입력값은 FastText 임베딩(100차원)을 사용했다. FastText 임베딩은 Word2Vec의 개선된 버전이며 59만 건에 이르는 한국어 문서를 미리 학습한 모델학습 데이터는 다음과 같다 이 영화 꿀잼 + 긍정 positive 이 영화 노잼 + 부정 negative 전이 학습 모델은 문장을 입력받으면 해당 문장이 긍정인지 부정인지를 출력한다. 문장을 형태소 분석한 뒤 각각의 형태소에 해당하는 FastText 단어 임베딩이 모델의 입력값이 된다. 위의 그래프로 처음 부터 하는 것 보다 FastText 임베딩을 사용한 모델의 성능이 좋다. 즉, 임베딩의 품질이 좋으면 수행하려는 Task의 성능 역시 올라간다. 왜 임베딩이 중요한지 깨달았다","link":"/2020/03/29/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%98%EC%97%AD%ED%95%A0/"},{"title":"단어가어떤순서로쓰였는가","text":"2.3 단어가 어떤 순서로 쓰였는가2.3.1 통계 기반 언어 모델언어 모델 ** language model**이란 단어 시퀀스에 확률을 부여하는 모델이다.단어의 등장 순서를 무시하는 백오브워즈와 달리 언어 모델은 시퀀스 정보를 명시적으로 학습한다. 단어가 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률, 즉 P라는 것을 반환한다. 통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다. 이렇게 되면 주어진 단어 시퀀스 다음 단어는 무엇이 오는게 자연스러운지 알 수 있다. n-gram이란 n개 단어를 뜻하는 용어이다. 난폭,운전 눈, 뜨다 등은 2-gram 또는 bigram이라는 말을 쓴다. 누명, 을, 쓰다 는 3-gram 혹은 trigram이라고 쓴다. 경우에 따라서 n-gram은 n-gram에 기반한 언어 모델을 의미하기도 한다. 말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 뜻이다. 예컨데 내, 내 마음 말뭉치는 빈도가 많지만 내 마음 속에 영원히 기억될 최고의 명작이다 라는 말뭉치가 한 번도 없을 수 있다. 이럴 때에는 말뭉치로 학습한 언어 모델은 해당 표현이 나타날 확률을 0으로 부여하게 된다. 문법적으로나 의미적으로 결함이 없는 훌륭한 한국어 문장임에도 해당 표현을 말이 되지 않는 문장으로 취급할 수 있다는 것이다. 내 마음 속에 영원히 기억될 최고의 라는 표현 다음에 명작이다라는 단어가 나타날 확률은 조건부확률 ** conditional probability**의 정의를 활용해 ==최대우도추정법== 으로 유도한다. $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) = { Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다) \\over Freq(내, 마음, 속에, 영원히, 기억될, 최고의)}$$ 그러나 우변의 분자가 0이라서 전체 값은 0이된다. 앞에서 배운 n-gram을 사용해보자. 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사하는 것이다. 이말을 다시 해석하면 한 상태** state의 확률은 그 직전 상태에만 영향을 받는 것이다. 마코프 가정 **Markov assumption에 기반한 것이다. $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) ≈ P(명작이다|최고의)={ Freq(최고의, 명작이다) \\over Freq(최고의)}$$ 처럼 전 상황에 대해서만 영향을 주는 것이다. 다시말해 명작이다 라는 직전의 1개 단어만 보고 전체 단어 시퀀스 등장 확률을 근사한 것이다. 좀 더 늘려서 끝까지 계산하게 되면 다음과 같다. $$P(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다) ≈ P(내)P(마음|내)P(속에|마음)P(영원히|속에)P(기억될|영원히)P(최고의|기억될)P(명작이다|최고의)$$ 일반화를 시킨다면 다음과 같다. 바이그램모델에서는 1개만 참고하지만 일반화를 시키면 전체 단어 시퀀스 등장 확률 계산시 직전 n-1개 단어의 히스토리를 본다 $$P(Wn|W(n-1) = { Freq(W(n-1,W(n)) \\over Freq(W(n-1))}$$ 그러나 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 문제가 발생할 수 있다. 처음 보는 단어를 본다면 그 확률은 0으로 보기 때문이다. 이를 위해서 백오프** back-off, 스무딩 ** smoothing등의 방식이 제안된다.백오프란 n-gram등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식인데, n을 크게 하면 할 수록 등장하지 않은 케이스가 많아질 가능성이 높기 때문이다.내 마음 속에 영원히 기억될 최고의 명작이다는 7-gram에서는 0이지만 N을 4로 내린다면 달라진다.스무딩이란 등장 빈도 표에 모두 K만큼 더하는 것이다. 높은 빈도를 가진 문자열 등장 확률을 일부 깎고 학습 데이터에 전혀 등장하지 않은 케이스들에는 일부 확률을 부여하게 된다. 2.3.2 뉴럴 네트워크 기반 언어 모델뉴럴 네트워크는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로 기능이 가능하기 때문에 뉴럴 네트워크로 사용한다. 1발 없는 말이 -&gt; [언어모델] -&gt; 천리 뉴럴 네트워크 기반 언어 모델은 위 그림처럼 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습된다. 학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용한다. 대표적인 모델은 다음과 같다. ELMo GPT 마스크 언어 모델 ** masked language model**은 언어 모델 기반 기법과 큰 틀에서 유사하지만 디테일에서 차이가 잇다. 문장 중간에 ‘마스크’를 씌워 놓고 해당 위치에 어떤 단어가 올지 예측하는 과정을 학습한다. 대게 언어 모델 기반 기법은 단어를 순차적으로 입력받아 다음 단어를 맞춰야하기 때문에 태생적으로 일방향 ** uni-directional이다. 하지만 마스크 언어 모델 기반 기법은 문장 전체를 보고 중간을 예측하기 때문에 양방향 ** bi-directional학습이 가능하다. 대표적인 모델은 다음과 같다. BERT","link":"/2020/04/02/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EB%8B%A8%EC%96%B4%EA%B0%80%EC%96%B4%EB%96%A4%EC%88%9C%EC%84%9C%EB%A1%9C%EC%93%B0%EC%98%80%EB%8A%94%EA%B0%80/"},{"title":"자연어계산과이해","text":"2.1 자연어 계산과 이해컴퓨터는 자연어를 사람처럼 이해할 수 없다. 그러나 임베딩을 활용하면 컴퓨터가 자연어를 계산하는 것이 가능해진다. 임베딩은 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과이기 때문이다. 컴퓨터는 임베딩을 계산/처리해 사람이 알아들을 수 있는 형태의 자연어로 출력한다. 자연어의 통계적 패턴 ** statistical pattern** 정보를 통째로 임베딩에 넣는다.임베딩을 만들 때 쓰는 통계 정보는 3가지가 있다. 문장에 어떤 단어가 많이 쓰였는지 단어가 어떤 순서로 등장하는지 문장에 어떤 단어가 같이 나타났는지 구분 백오프워즈 가정 언어 모델 분포가정 내용 어떤 단어가 많이 쓰였는가 단어가 어떤 순으로 쓰였는가 어떤 단어가 같이 쓰였는가 대표 통계량 TF-IDF - PMI 대표 모델 Deep Averaging Network ELMo, GPT Word2Vec 언어 모델에서는 단어의 등장 순서를, 분포 가정에서는 이웃 단어를 우선시한다. 어떤 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 뗄래야 뗄 수 없는 관계를 가진다.한편, 분포 가정에서는 어떤 쌍이 얼마나 자주 나타나는지와 관련한 정보를 수치화하기 위해 개별 단어 그리고 단어 쌍의 빈도 정보를 적극 활용한다. 백오브워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다.","link":"/2020/03/31/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%9E%90%EC%97%B0%EC%96%B4%EA%B3%84%EC%82%B0%EA%B3%BC%EC%9D%B4%ED%95%B4/"},{"title":"환경소개","text":"1.4.1 환경소개 Ubuntu 16.04.5 Python 3.5.2 Tensorflow 1.12.0 도커 구성하기본인은 Synology NAS를 사용하고 있으므로 NAS에서 docker 환경을 구성하는 방법을 포스팅하려고 한다.혹 NAS가 없는 경우 책에는 AWS로 하는 방법을 소개했으니 참고하자.도커이미지 123$ uname -a #ubuntu 환경 확인$ pwd #위치 확인$ cd /home #home으로 이동 1234root@docker-NLG:/# apt upgrade root@docker-NLG:/# apt update root@docker-NLG:/# apt install python3.5.2root@docker-NLG:/# apt install python3-pip pip3 라고 command에 쳤을 때 뭐라뭐라 길게 나오면 성공!더 정확하게 확인하려면 12root@docker-NLG:/# pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) 123root@docker-NLG:/# pip3 install --upgrade piproot@docker-NLG:/# pip --version pip 20.0.2 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6) 도커 다운을 위한 패키지 설치안된다면 앞에 sudo를 붙여보시길.. 1234apt install apt-transport-https apt install ca-certificatesapt install curl apt install software-properties-common apt-transport-https : 패키지 관리자가 https를 통해 데이터 및 패키지에 접근할 수 있도록 한다.ca-certificates : ca-certificate는 certificate authority에서 발행되는 디지털 서명. SSL 인증서의 PEM 파일이 포함되어 있어 SSL 기반 앱이 SSroot@docker-NLG:/# pip3 –version L 연결이 되어있는지 확인할 수 있다.curl : 특정 웹사이트에서 데이터를 다운로드 받을 때 사용software-properties-common : *PPA를 추가하거나 제거할 때 사용한다. curl 명령어로 도커 다운받기 &amp;&amp; repository에 경로 추가하기1234root@docker-NLG:/# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - OK root@docker-NLG:/# add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&quot; curl 명령어의 옵션 f : HTTP 요청 헤더의 contentType을 multipart/form-data로 보낸다.s : 진행 과정이나 에러 정보를 보여주지 않는다.(–silent)S : SSL 인증과 관련있다고 들었는데, 정확히 아시는 분 있다면 댓글 부탁!L : 서버에서 301, 302 응답이 오면 redirection URL로 따라간다.apt-key : apt가 패키지를 인증할 때 사용하는 키 리스트를 관리한다. 이 키를 사용해 인증된 패키지는 신뢰할 수 있는 것으로 간주한다. add 명령어는 키 리스트에 새로운 키를 추가하겠다는 의미이다. add-apt-repository : PPA 저장소를 추가해준다. apt 리스트에 패키지를 다운로드 받을 수 있는 경로가 추가된다. docker 패키지가 검색되는지 확인1234root@docker-NLG:/# apt-get update root@docker-NLG:/# apt-cache search docker-ce docker-ce-cli - Docker CLI: the open-source application container engine docker-ce - Docker: the open-source application container engine apt update : 저장소의 패키지 갱신 도커 설치하기 &amp;&amp; ubuntu를 도커그룹으로 입력123root@docker-NLG:/# apt-get install docker-ceroot@docker-NLG:/# usermod -aG docker $USER Usage: usermod [options] LOGIN nvidia-docker 설치위의 단계까지 끝내면 일반적인 도커 기능들을 이용하실 수 있습니다. 하지만 NVIDIA의 GPU를 이용하시면서 여러 환경의 CUDA Tookit을 이용하실 경우 nvidia-docker라는 확장 기능을 추가하시면 보다 편리하게 사용하실 수 있습니다. nvidia-docker를 설치하고자 하실 경우 호스트 운영체제에 먼저 NVIDIA 드라이버가 설치되어 있어야 합니다. NVIDIA의 그래픽카드 또는 GPU를 사용하지 않는 경우 이 과정을 진행하고 도커 설치과정을 끝내실 수 있습니다. 우분투에서 NVIDIA 드라이버 설치 방법12345678910root@docker-NLG:/# curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey|apt-key add - OKroot@docker-NLG:/# distribution=$(. /etc/os-release;echo $ID$VERSION_ID) root@docker-NLG:/# curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list|tee /etc/apt/sources.list.d/nvidia-docker.list deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /root@docker-NLG:/# apt-get update nvidia-docker 실행우분투에서 nvidia-driver받기 1234root@docker-NLG:/# apt-get install -y nvidia-container-toolkitroot@docker-NLG:/# apt-get install systemdroot@docker-NLG:/# systemctl restart dockerroot@docker-NLG:/# docker run --gpus all nvidia/cuda:9.0-base nvidia-smi 도커실행하기1root@docker-NLG:/home# git clone https://github.com.rastgo/embedding 참고 링크 오픈소스링크TensorFlow : https://www.tensorflow.orgGensim : https://radimrehurek.com/gensimFastText : https://fasttext.ccGloVe : https://nlp.stanford.edu/projects/gloveSwivel : https://github.com/tensorflow/models/tree/master/research/swivelELMo : https://allennlp.org/elmoBERT : https://github.com/google-research/bertScikit-Learn : https://scikit-learn.orgKoNLPy : http://konlpy.org/en/latest/Mecab : http://eunjeon.blogspot.com/soynlp : https://github.com/lovit/soynlpKhaiii : https://github.com/kakao/khaiii https://tech.kakao.com/2018/12/13/khaiii/Bokeh : https://docs.bokeh.org/sentencepiece : https://github.com/google/sentencepiece","link":"/2020/03/30/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%ED%99%98%EA%B2%BD%EC%86%8C%EA%B0%9C/"},{"title":"아나콘다설치","text":"아나콘다 가상 환경 구성아나콘다가 설치 되었다는 가정하에 진행하겠다파이썬 프로젝트는 파이썬 실행 환경을 독립적으로 가능하게 해주므로 가상 환경을 만들어 보자.conda 명령어를 통해 가상 환경을 생성하자. 12lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ conda create --name pr_tensorflow python=3 conda crate는 가상환경을 생성하는 것을 의미–naeme 옵션에 pr_tensorflow입력하고 3버전을 입력 123lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ conda activate pr_tensorflow(pr_tensorflow 윈도우 환경에서는 activate로 가상환경을 실행해주면 (pr_tensorflow)가 만들어진다 가상환경이름을 잊어버렸다면 123456lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ conda env list# conda environments:#base C:\\Users\\lego7\\Anaconda3pr_tensorflow * C:\\Users\\lego7\\Anaconda3\\envs\\pr_tensorflow 실습 환경 구성github링크 123456789101112131415lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ git clone https://github.com/NLP-kr/tensorflow-ml-nlp.gitCloning into 'tensorflow-ml-nlp'...lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ lstensorflow-ml-nlp/(pr_tensorflow)lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG$ cd tensorflow-ml-nlp(pr_tensorflow)lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG/tensorflow-ml-nlp (master)$ ls1.Intro/ 3.NLP_INTRO/ 5.TEXT_SIM/ main.png requirements.txt2.NLP_PREP/ 4.TEXT_CLASSIFICATION/ 6.CHATBOT/ README.md(pr_tensorflow) python 3.6 버전을 설치해주자 12lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG/tensorflow-ml-nlp (master)$ conda install python=3.6 123lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG/tensorflow-ml-nlp (master)$ pip install -r requirements.txtCollecting tensorflow 주피터노트북 실행 12lego7@DESKTOP-UHCO554 MINGW64 ~/Desktop/NaverCloud/hyeonukdev/NLG/tensorflow-ml-nlp (master)$ jupyter notebook","link":"/2020/04/01/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4%EC%84%A4%EC%B9%98/"},{"title":"어떤단어가같이쓰였는가","text":"2.4 어떤 단어가 같이 쓰였느가2.4.1 분포 가정자연어 처리에서 분포 distribution란 특정 범위, 즉 ++윈도우++ 내에 동시에 등장하는 ++이웃 단어++ 또는 ++문맥의 집합++을 가리킨다. 개별 단어의 분포는 그 단어가 문장 내에서 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라진다. 어떤 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 도한 유사할 것이라는 게 분포 가정 ** distributional hypothesis**의 전제다. 예컨데 한국어의 빨래, 세탁이라는 단어의 의미를 전혀 모른다고 하자. 두 단어의 의미를 파악하기 위해서는 이들 단어가 실제 어떻게 쓰이고 있는지 관찰을 해야한다. 두 단어는 타깃 단어 ** target word**이고 청소, 물 등은 그 주위에 등장한 문맥 단어가 된다 특기는 자칭 청소와 빨래지만 요리는 절망적재를 우려낸 물로 빨래 할 때 나찬 물로 옷을 세탁한다.세탁, 청소, 요리와 기사는 이웃한 단어들이 서로 비슷하기 때문이다. 빨래가 청소 물 과 같이 등장하는 경향을 미루어 짐작해볼 때 이들끼리도 직간접적으로 관계를 지닐 가능성이 낮아보이지는 않는다. 그럼에도 개별 단어의 분포 정보와 그 의미 사이에는 논리적으로 직접적인 연관성은 사실 낮다. 다시 말해 분포 정보가 곧 의미라는 분포 가정에 의문접이 발생한다. 2.4.2 분포와 의미(1) : 형태소형태소** morpheme란 의미를 가지는 최소 단위를 말한다. 더 쪼개면 의미를 잃어버리는 것이다.예를 들어 철수가 밥을 먹었다 라고 한다면 형태소 후보는 철수, 밥, 이다.조금 더 깊게 분석해보자. 계열관계 ** paradigmatic relation가 있다. 계열 관계는 해당 형태소 자리에 다른 형태소가 ‘대치’되어 쓰일 수 있는 가를 따지는 것이다. 예컨데 철수 대신에 영희가 올 수 있고 밥대신 빵을 쓸 수 있다. 이를 근거로 형태소 자격을 부여한다. 특정 타깃 단어 주변의 문맥 정보를 바탕으로 형태소를 확인한다는 이야기와 일맥상통한다. 말뭉치의 분포 정보와 형태소가 밀접한 관계를 이루고 있다. 2.4.3 분포와 의미(2) : 품사품사란 단어를 문법적 성질의 공통성에 따라 언어학자들이 몇 갈래로 묶어 놓은 것이다. 기능 의미 형식위 세가지를 기준으로 분류한다. 기능한 단어가 문장 가운데서 다른 단어와 맺는 관계를 가르킨다.깊이 높이는 문장에서 주어로 쓰이고 깊다 높다는 서술어로 사용되고 있다. 의미단어의 형식적 의미를 나타낸다. ``깊이높이를 하나로 묶고깊다높다`를 같은 군집으로 넣을 수 있다. 품사에서는 어휘적 의미보다 형식적 의미가 중요하다. 다시말해 어떤 단어가 사물의 이름을 나타내는가, 그렇지 않으면 움직임이나 성실, 상태를 나타내느냐 하는 것이다. 형식단어의 형태적 특징을 의미한다. 깊이 높이는 변화하지 않는다. 깊었다 높았다 깊겠다 높겠따 따위와 같이 어미가 붙어 여러 가지 모습으로 변화를 일으킬 수 있다.그러나 예외가 있다. 공부 공부하다 두 개를 분류하려면 공부는 명사이지만 우리는 동작이라는 여지를 알고 있다. 품사 분류에서 가장 중요한 기준은 ==기능==이다. 해당 단어가 문장 내에서 점하는 역할에 초점을 맞춰 품사를 분류한다는 것이다.형태소의 경계를 정하거나 품사를 나누는 것과 같은 다양한 언어학적 문제는 말뭉치의 분포 정보와 깊은 관계를 갖고 있다. 이로인하여 분포 정보를 함축한다면 해당 벡터에 해당 단어의 의미를 자연스레 내재시킬 수 있는 것이다. 2.4.4 점별 상호 정보량점별 상호 정보량은 두 확률변수사이의 상관성을 계량화하는 단위다. 두 확률변수가 완전히 독립인 경우 그 값이 0이 된다. 독립이라고 하면 A가 나타나는 것이 단어 B의 등장할 확률에 전혀 영향을 주지 않고, 단어 B등장이 단어 A에 영향을 주지 않는 경우를 가리킨다.두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화한 것이다$$PMI(A,B) = log{P(A,B) \\over P(A)*P(B)}$$ PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수도 있다.","link":"/2020/04/02/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/%EC%96%B4%EB%96%A4%EB%8B%A8%EC%96%B4%EA%B0%80%EA%B0%99%EC%9D%B4%EC%93%B0%EC%98%80%EB%8A%94%EA%B0%80/"},{"title":"2장요약","text":"2.5 2장 요약 벡터가 어떻게 의미를 가지게 되는가 임베딩에 자연어의 통계적 패턴 정보를 주면 자연어의 의미를 함축할 수 있다. 백오브워즈 가정에서는 어떤 단어의 등장 여부 혹은 그 빈도 정보를 중시한다. 백오브워즈 가정의 대척점에는 언어 모델이 있다. 언어 모델은 단어의 등장 순서를 학습해 주어진 단어 스퀀스가 얼마나 자연스러운지 확률을 부여한다. 분포 가정에서는 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다. 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다.","link":"/2020/04/02/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/2%EC%9E%A5%EC%9A%94%EC%95%BD/"},{"title":"빅데이터(Big Data) 분석의 중요성","text":"데이터로부터 얼마나 많은 부가가치를 얻을 수 있나? 과학 기술 경쟁이 치열 침단 과학기술이 사용자의 요구를 충족시켰을때 부가가치 창출(아마존 마케팅, 애플 등) 미래 사용자의 필요 예측이 매우 중요 기술분야에서 앞으로 중시될 세가지 영역 데이터 마이닝, 기계학습, 인공지능, 자연어처리 Business Intelligence, 경쟁정보전략 분석, 통계 빅데이터의 시대적 변화o 1990년대 중반 ~ 2000년대 중반 고객정보 유통사 상품 구매정보 상품/서비스 구매정보 교통정보 리서치 정보 o 2000년대 중반 ~ 2010년대 중반 Data의 통합 관리를 통한 소비자의 입체적 이해 o 2010년대 중반 이후 고객정보 모바일 SNS 이종 데이터 간의 결합 Cloud Computing 데이터 마켓의 출현 빅데이터 활용 개념도 빅데이터 산업 분류 빅데이터 활용사례 공공빅데이터 구글트렌드 빅데이터 활용 프로세스 데이터수집 및 관리 정형데이터 - 고객수 - 판매량 - 키워드빈도 비정형데이터 - 로그 - SNS 텍스트 - 사진 데이터분석 기초통계분석 - 집계 - 통계치 - 회귀분석 데이터마이닝/기계학습/AI/딥러닝 - 군집,연관 - 시계열, 추천 - 텍스트마이닝 - 커뮤니티분석 데이터활용 시각화 신제품전략 마케팅전략 니즈발견 리스크경감 데이터분석에 필요한 기술들 Mathematics Expertise 통계 선형대수 미분/적분 Business/Strategy Acumen 도메인 지식/경험 문제 영역 Technology, Hacking Skills 컴퓨터 프로그래밍 데이터베이스 인프라 시스템 빅데이터 인프라 : 하둡 하둡은 오픈소스 분산처리 기술 하둡 분산 파일 시스템 HDFS(Hadoop Distributed File System) 간단한 서버들을 이용하여 가상화된 HDFS을 구성하고 여기에 존재하는 거대한 데이터를 간편하게 다루는 MapReduce 프레임워크를 구현하여 제공 빅데이터 관련 이슈 개인정보 이슈 인력양성 빅데이터의 지도화 자료의 품질관리 중요성 수학 및 통계, IT, 언어학, 비즈니스 등의 다양한 영역의 융합교육 필요 인프라 또는 IT기술 뿐만 아니라, 분석과 시각화 및 해석의 중요성 부각 장기적인 관점에서의 데이터 분석 및 관리 필요","link":"/2020/04/15/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0(Big%20Data)%EB%B6%84%EC%84%9D%EC%9D%98%EC%A4%91%EC%9A%94%EC%84%B1/"},{"title":"빅데이터(Big Data)의 이해","text":"빅데이터 등장 배경 디지털 기술 발전에 따른 대규모 데이터 생성 데이터 생성 규모(Volume)확대 생성주기(Velocity)가 짧아짐 기존 수치 위주에서 문자, 이미지, 영상 등 다양한 형태(Variety)로 발생 빅데이터의 정의 명확히 합의된 정의가 없음 - 데이터 수집, 저장, 관리, 분석을 처리하는 통상의 용량을 넘어서는 Dataset 규모로, 그 정의는 주관적이며 앞으로도 기술발전에 따라 정의는 변화될 것이다. - 가장 많이 쓰이는 정의 3V + Value - 엑셀로 처리하기에 적합하지 않는 데이터 - SPSS, SAS와 같은 통계 패키지 등에 적합한 자료의 의미 3V + Value 빅데이터의 분류 및 처리과정 데이터 마이닝(Data Mining) 데이터베이스 내에서 어떠한 방법에 의해 관심 있는 지식을 찾아내는 과정 - 대용량의 데이터 속에서 유용한 정보를 발견하는 과정이며 이러한 기술을 의미 - 데이터 베이스 마케팅 분야 - 순차패턴, 유사성을 활용 정의 - 복잡한 통계적인 분석이나 모형구축 기법을 통해 대용량의 데이터 내에 이전에는 알려지지 않았던 패턴이나 규칙 등을 탐색하고 모형화 하여 유용한 지식을 추출하는 일련의 과정 - 통계적 관점 : 대용량의 데이터에 대한 탐색적 데이터 분석(Exploratory Data Analysis) 빅데이터 환경의 특징 구분 기존 빅데이터 환경 데이터 정형화된 수치자료 중심 -비정형의 다양한 데이터 - 문자 데이터 - 영상 데이터 - 위치 데이터 하드웨어 고가의 저장장치 데이터베이스 데이터웨어하우스 - 클라우드 컴퓨팅 등 비용 효율적인 장비 활용 가능 소프트웨어/분석 방법 관계형 데이터베이스 통계패키지 데이터마이닝 머신러닝 - 오픈소스 형태의 무료 소프트웨어 - Hadoop, NoSQL - 오픈소스 통계솔루션(R) - 텍스트 마이닝 - 온라인 버즈 분석 -감성 분석 일반데이터와 빅데이터의 차이 빅데이터 IT 핵심 기술","link":"/2020/04/15/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0(Big%20Data)%EC%9D%98%EC%9D%B4%ED%95%B4/"},{"title":"통계의 목적과 엑셀 데이터 기초","text":"통계학의 정의통계학(statistics)은 수량적인 비교를 기초로 사실을 관찰하고 분석하는 방법을 연구하는 학문일반적으로 수집되는 데이터가 조사자, 시기, 방법, 목적 등에 따라 다르게 나타나는 불균형적인 데이터이지만, 통계학은 이 안에서 의미를 찾아내고 실생활에서 적용가능한 유용성을 찾아내 이를 수치로 표현할 수 있다. 기술통계(descriptive statistics):표본에 대한 분석 결과의 각종 수치들을 활용하여 집단의 특성을 설명 추론통계(inference statistics):표본을 활용하여 모집단의 특성을 나타내는 것 통계학의 목적 의사결정많은 정보를 지각하고 평가하여 하나를 선택 정보와 반응 사잉의 다대일 대응으로 나타남 여러가지 대안 가운데 하나를 선택할 때 사용 불확실성의해소의사결정을 하게 되면 그 결과가 정확한 것이라 할 수 있는가의 문제 빅데이터의 개념을 들여와 불확실성을 해소 정보수집이 어려움 시장의 변화와 대응의 어려움에대한 극복 필요 요약다양한 데이터를 신속히 이해할 수 있도록 다양한 형태로 표현 불확실성의 감소를 위해 사용 반복되어 생산되는 데이터를 정리된 보고서로 표현하여 불확실성이 낮은 상황의 의사결정이 가능하도록 함 연관성 파악요약된 보고서에서 주요한 항목들 간의 연관성을 파악한 경쟁우위의 확보 의사결정권자에게 항목 간 연관성을 제시해 미래의 계획을 지원 다양한 자료는 의사결정에 있어 세부적 판도에 기여 예측인과관계 파악을 통해 패턴을 찾아내고 이러한 패턴을 통해 추세를 판단 다양한 변수의 대입과 삭제를 통해 예측 가능 통계분석의 과정통계분석은 표본을 통해 의미 있는 자료를 추출하고, 이를 기반으로 의사결정, 불확실성의 해소요약, 연관성 파악, 예측 등의 결과로 이어지도록하는 일련의 과정이다==수집 -&gt; 정제 -&gt; 추정 -&gt; 검정== 수집 조사과정에서 자연스럽게 수집 조사자가 특정 목적에 맞는 자료를 얻기 위해 설계한 수집 도구를 이용보통 자료가 많을 수록 통계분석에서 더 좋은 결과를 얻을 것이라고 생각하지만 조사의 목적에 맞는 적절한 자료를 수집해서 최적의 통계방법으로 분석한 결과가 가장 정확도가 높음 정제 분석에 적합한 자료를 선별 적합하지 않은 자료는 삭제 추정 통계 분석을 진행하는 것 -&gt; 모수를 추정하는 것 표본의 특성을 설명하는 통계량을 통해 모집단의 모수를 추정하게 됨 검정 통계조사의 목적 -&gt; 주장이 믿어지는 사실이 실제로 옳은지 아닌지를 확인 수립된 가설이 유의미하고 타당성을 가지는지 통계적으로 확인하는 과정 확인을 통해 가설의 기각와 채택여부를 판단 통계분석의 한계 확률이 없으면 무의미 - 통계분석의 결과는 항상 확률과 연관되어 표현 항상 틀릴 가능성 내포 - 신뢰수준 100% -&gt; 신뢰구간에 해당하는 값 (-무한 ~ + 무한) - 결과의 범위가 줄수록 신뢰구간은 100%에서 멀어짐","link":"/2020/04/15/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%ED%86%B5%EA%B3%84%EC%9D%98%EB%AA%A9%EC%A0%81%EA%B3%BC%EC%97%91%EC%85%80%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B8%B0%EC%B4%88/"},{"title":"엑셀 수식 사용 - 1강. 수식과 함수","text":"1.수식의 이해 Cell의 자료형식 숫자형식 : 0 ~ 9 숫자의 정밀도는 앞에서 15자리, -9.9E+307~9.9E+307 범위 문자형식 : A ~ Z, a ~ z, 가나다라, 특수문자 수학형식 : = 으로 시작 날짜/시간 형식 수식 - 연산자(수식연산자, 논리연산자) 수식연산자(Mathematical operators) 연산자 설명 + 더하기 - 빼기 * 곱하기 / 나누기 ^ 지수 &amp; 문자열조합 논리연산자(Logical operators) 연산자 설명 = 같음 &gt; 보다 큼 &lt; 보다 작음 &gt;= 보다 크거나 같음 &lt;= 보다 작거나 같음 &lt;&gt; 같지 않음 연산자 우선 순위(Operator precedence) 기호 연산자 연산순위 ^ 지수승 1 * , / 곱하기,나누기 2 + , - 더하기,빼기 3 &amp; 문자열 결합 4 = , &lt;,&gt; 같음,작음,큼 5 예시","link":"/2020/04/16/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EC%97%91%EC%85%80%EC%88%98%EC%8B%9D%EC%82%AC%EC%9A%A9-1%EA%B0%95%EC%88%98%EC%8B%9D%EA%B3%BC%ED%95%A8%EC%88%98/"},{"title":"엑셀 수식 사용 - 2강. 함수 사용법","text":"2.함수 사용법 함수 함수의 형태 함수명() 괄호 내부에는 변수(or 인자) 사용 변수 변수에는 상수, 주소, 범위, 이름, 함수 등 사용 변수 없는 함수 존재 ex) TODAY(), PI() 함수는 수식의 일부분 엑셀에는 400개 이상의 함수 존재 직접만들기도 가능 함수 입력방법 수동 입력 방식 ‘=’ 과 함수명 입력 후 괄호열기 입력 함수명 입력 중 아래 리스트에서 선택 후 탭 키 입력 빠른 함수 입력 [수식] 리본메뉴 - ‘함수라이브러리’ [홈] 리본메뉴 - [자동합계] FX 메뉴 OR Shift-F3 입력 함수마법사 함수 구문 이해 변수 구분 굵은 글씨 : 현재 입력 중인 변수 위치 대괄호[] : 필수가 아닌 옵션 “…” : 변수를 더 사용 가능 자주 쓰는 변수 표기 number : 숫자 range : 범위 crieria : 조건문 lookup_value : 찾을값 lookup_array : 찾을 범위 logical_test : 논리비교 text : 문자열 num_chars : 글자수 셀 참조(references) 상대참조 기본 참조방식 수식을 복사하면, 수식 내 셀 주소가 행과 열 방향으로 이동한 만큼 셀 주소가 변함 절대참조 행과 열 주소 앞에 “$” 문자 표시 절대 참조는 수식에 복사되어도 참조 위치는 변화 없음 혼합참조 행과 열 주소 둘 중 한 군데 앞에 “$” 표시 상대참조와 절대참조가 혼합 “F4” 상대참조 -&gt; 절대참조 -&gt; 혼합참조(행) -&gt; 혼합참조(열) -&gt; 상대참조 함수 에러 메시지 DIV/0! 수식에 0으로 나누는 내용이 있음, 대게 분모 값에 빈셀이 지정됨 NAME? : 수식 엑셀에 인식할 수 없는 이름이 사용됨, 대게 이름이 지워지거나 텍스트 입력시 큰따옴표 오류 N/A : 수식에 사용할 수 없는 데이터가 지정된 경우, 일부함수에서 오류 NULL! : 수식이 교집합이 없는 두 범위의 교집합을 설정함 NUM! : 값 에러, 양이 되어야하는 값에 음의 값이 입력될 때 REF! : 수식이 참조할수 없는 셀을 지정할 때, 워크시트 내에 삭제된 셀이 사용될 때 VALUE! : 수식에 잘못된 형태의 변수나 숫자가 포함되어 있을 때","link":"/2020/04/16/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EC%97%91%EC%85%80%EC%88%98%EC%8B%9D%EC%82%AC%EC%9A%A9-2%EA%B0%95%ED%95%A8%EC%88%98/"},{"title":"소프트스킬(1)","text":"다른 이들과 달리 멋지게 시작하기 사업가처럼 생각하기 나는 어떤 제품이나 서비스를 팔 생각인가? 서비스 개선 방법을 고민하기 고정관념에서 벗어나 사업가처럼 사고하자 목표를 설정하고 미래에 대비하기 성취할 목표를 설정하기 목표를 정확히 이해하기 크거나 구체적인것 보다 방향을 제시해야한다 목표를 주기적으로 살피자 시간을 내서 큰 목표를 최소 하나 이상을 기록하기 큰 목표를 월간, 주간, 일간으로 작은 목표로 나누기 큰 목표를 매일 생각할 수 있도록 잘 보이는 장소에 두기 면접의 달인이 되기 고정관념에서 벗어나 인맥 쌓기 개발자들의 블로그를 찾아 친부 쌓기 기술 수준을 꾸준히 발전시키기 전문성을 갖추기 전문성으로 많은 기회의 문이 열린다 전문성이 높아질수록 잠재적 기회가 줄어드는 반면 기회를 잡을 확률은 점점 높아진다 전문분야는 다음과 같다 - 웹 개발 기술 - 임베디드 시스템 - 특정 운영체제 - 모바일 개발 - 프레임워크 - 소프트웨어 시스템 승진하기 존재감 있게 일하기 주간 보고서를 작성하기 자신의 활동을 매일 기록하기 어떤 문제든 해결책을 제시할 수 있는 사람이 되기 해결책을 실행할 수 있는 사람이 되기 전문가되기 사고방식 전환하기 약속을 지키고 맡은 일을 완수하고 포기하지 않기 맡은 일과 경력을 진지하게 생각하기 좋은 습관 기르기 - 시간 관리 기술 자기 계발하기 첫 제품 만들기 문제 해결을 염두하고 제품을 제작하기 How to Market Yourself as a Software Developer 에릭리스 - 린스타트업 이루고 싶은게 있다면 이미 이룬 것처럼 연기하라이력서쓰기 온라인 이력서 만들기 개성을 더하기 행동과 결과를 연결하기 교정하기","link":"/2020/04/16/books/%EC%86%8C%ED%94%84%ED%8A%B8%EC%8A%A4%ED%82%AC_1/"},{"title":"순열과 조합","text":"순열(Permutation)-&gt; PERMUT(number, number_chosen) 서로 다른 n개의 원소에서 r개를 중복없이 골라 순서에 상관 있게 나열하는 것으로 n개에서 r개를 택하는 순열이라고 함 $$ nPr = n * (n-1) * (n-2) * … * (n - r + 1) = n! / (n-r)! $$ 순열의 종류 순열 중복순열 : n개에서 r개를 순서에 상관 있게 뽑는데 중복가능 등차순열 : n에서 r개를 뽑는데, n개 중에서 똑같은 것이 섞인 경우 원순열 : n개를 원형으로 나열하는 경우 조합(Combination)-&gt; COMBIN(number, number_chosen) 서로 다른 n개의 원소에서 순서에 상관없이 r개를 뽑을 때, 이때 n개에서 r개를 택하는 조합 $$ nCr = nPr / r! = n!/ (n-r)!r! $$ 조합의 종류 조합 중복조합 : n개에서 r개를 순서에 상관없이 뽑는데 중복을 허락하는 경우","link":"/2020/04/18/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EC%88%9C%EC%97%B4%EC%A1%B0%ED%95%A9/"},{"title":"데이터 자료형","text":"python 자료형 자료형 저장 모델 변경 가능성 접근방법 수치형 int, float, complex Literal Immutable Direct 문자열 str Container Immutable Sequence 튜플 tuple Container Immutable Sequence 리스트 list Container Mutable Sequence 사전 dict Container Mutable Mapping 집합 set Container Mutable set 저장 모델 Literal : 단일 종류 Container : 종류에 무관 변경 가능성 Immutable : 변경 불가 Mutable : 변경 가능 접근 방법 Direct : 직접 할당 Sequence : 순서 중시 Mapping : 순서 무관 Set : 중복 불가 튜플은 최초에 입력한 데이터 변경이 불가 리스트는 최초에 입력한 데이터 변경 가능 사전은 순서에는 상관 없지만 중복 가능 집합은 중복 불가능 C자료형 자료형 메모리크기 데이터 범위 정수형 char 1바이트 -128 ~ +127 정수형 short 2바이트 -32768 ~ +32767 정수형 int 4바이트 -2147483648 ~ +2147483647 정수형 long 4바이트 -2147483648 ~ + 2147483647 실수형 float 4바이트 3.4x10^-37 ~ +3.4x10^+38 실수형 double 8바이트 1.7x10^-307 ~ 1.7x10^+308 실수형 long double 8바이트 이상 -","link":"/2020/04/21/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9E%90%EB%A3%8C%ED%98%95/"},{"title":"Background of NLP","text":"Basic Concept of Machine Learning 예를 들어 호랑이와 고양이를 예측하는 인공지능을 만들고 싶다라고 한다면 모델입장에서는 두가지가 필요하다 호랑이이미지 = 호랑이레이블 고양이이미지 = 고양이레이블 이렇게 레이블 정보(With Labels)를 활용해서 학습하는 것을 Supervised Learning 지도학습 이라고 한다 반면, 레이블 정보 없이(No Labels) 이미지만을 활용해서 하는 것을 Unsupervised Learning 비지도학습 이라고 한다 레이블정보가 아니라 입력이미지에 대해서 잘 분류하였으면 +1점, 잘 분류하지 못했으면 -1점 처럼 리워드를 주는 방식(With Rewards)을 Reinforcement Learning 강화학습 이라고 한다 Machine Learning Supervised Learning Regression : 수치형, 회기, 아파트가격/주가예측 Classification : 카테고리컬 variable, 클래스로 예측하는 것, 분류, 남성/여성, 양성/음성 Unsupervise Learning Clustering : 레이블이 없는 입력이미지를 바탕으로 학습할 때 유사하다고 계산이 되는 것을 군집화 하는 것 Dimensionality Reduction : 데이터가 엄청나게 큰 벡터로 표현될 때 적은차원의 벡터로 표현 하는 것, 차원축소의 기법 Reinforcement Learning Decision Process : 의사결정 Game Ai : 게임 ai Process of Machine Learning데이터 수집 -&gt; 해당 데이터를 전처리(해당 모델에 대해서 잘 이용할 수 있게 작업) -&gt; 모델링 -&gt; 학습한 모델의 성능을 평가 Data Collection Database Server API Preprocessing Not Available : 정형데이터의 경우 변수에 없는 값이 발생할 수 있다 Scaling : 변수 간의 정보 반영이 단위 값에 따라 다를 수 있다 ex) 사람의 키는 cm 몸무게는 kg 인데 남자/여자 판별할 때 영향력이 1cm와 1kg은 다르다 Derived Variables : 파생변수(각 변수를 조합)를 만든다 Modeling ML Model DL Model RL Model Performance Measure Accuracy : 얼마나 잘 분류했는지 Mean Squared Error : 얼마나 잘 예측했는지 오차 계산 Similarity : 얼마나 유사한지 Role of Train, Valid, Test Dataset잘 활용하기 위해서는 3가지의 Dataset이 필요하다 Train 학습을 하는데 이용한다 학습이 완료되고 예측을 해본다 Using for Training Model Valid 학습을 하지 않고 예측만 진행한다 만약 예측시 valid보다 train이 좋지 않으면 다시 학습한다 모니터링의 역할만 한다 Using for Training Direction Test 모델이 한 번도 보지 못한 것을 평가 Using for Measure Performance 10000의 dataset이 있을 경우 train : 6000, valid : 2000, test : 2000 으로 한다 8 : 1 : 1 도 한다 Process of NLP with Deep Learning자연어처리 딥러닝을 이해하자 Data Collection Crawling : html에서 데이터를 직접 가져오기 ex) 영화평을 크롤링해서 분석 Preprocessing Tokenizer Make Corpus Representation Modeling ML Model DL Model RL Model Performance Measure Accuracy Mean Squared Error Similarity Make Corpus with Train Dataset Make Number to Input Natural Language into Computer자연어를 숫자 값으로 변경하기 Make Token to Make a Dictionary토큰을 만들기 ‘I am a boy’ -&gt; ‘I’, ‘am’, ‘a’, ‘boy’ 띄어쓰기를 바탕으로 단어 분리 ‘I am a girl’ -&gt; ‘I’, ‘am’, ‘a’, girl’ 각 단어는 토큰을 의미 Make a Dictionary out of the Tokens in the Train Dataset트레인 데이터셋 안에 있는 토큰으로만 단어사전을 만든다 Replace Words with Numbers using a Dictionary숫자를 붙여주기 ‘I’ : 0, ‘am’ : 1, ‘a’ : 2, ‘boy’ : 3, ‘girl’ : 4 ‘I am a boy’ -&gt; [0,1,2,3] ‘I am a girl’ -&gt; [0,1,2,4] 겹치지 않는다 Representation Vector ‘boy’ : 3 , ‘girl’ : 4 boy and girl have similar roles in sentence남자와 여자는 문장에서 비슷한 역할을 수행할 것이다 But 3,4 can’t represent their role enough하지만 3,4만으로는 비슷하다는 것을 나타낼 수 없다 Represent Natural Language with Vectors not scalars원-핫벡터도 어떠한 인덱스만 1이기 때문에 스칼라로 보자다양한 숫자로 채워진 벡터로 만들어보자 Word2Vec, Glove, FastTest(Word) Sentence, Document, Sentence Piece단어 뿐만아니라 문장, 문서 등으로 토큰을 만들 수 있다","link":"/2020/04/25/NLP/BackgroundofNLP/"},{"title":"Vector for NLP","text":"라이브러리 소개Representation VectorBackground원-핫 인코딩을 표현한 벡터, 트레인 데이터 셋 안에 전체 문장들의 합을 코퍼스라고 했을 때, 코퍼스 안에서 어떤 토크나이저를 거쳐서 딕셔너리를 만든다 -&gt; v가 된다 Make several tokens from sentences Build a Dictionary of words by indexing each token Make a Vector(Dimension : Number of Words) 1 for the corresponding index with the remaining 0 Orthogonal Vector -&gt; Lose of Context InformationRome Paris는 역할이 비슷하지만 표현할 수 없다 Sparse Vector -&gt; Curse of Dimensionality벡터를 내적했을 때 값이 크면 유사도가 높지만 원-핫 인코딩 벡터는 1개 빼고 0 이 된다 두 가지 이유로 적합하지 않음 -&gt; 토크나이저를 쓰자 TokenizerEnglish -&gt; SPACY, NLTK한국어는 형태소라는 것이 있기 때문에 띄어쓰기로는 애매할 수 있다.이러한 것을 보완해서 한국어토큰이 별도 있다.Korean -&gt; KoNLPy(Hannanum, Kkma, Komoran, Twitter), MECAB, KHAIII 한국어의 사전에 따라 모델의 성능이 다르기 때문에 중요하다. 넘버링이 되어 있는 단어를 어떻게 표현할 것인지 알아보자 Word2Vec Efficient Estimation of Word Representations in Vector Space(Tomas Mikolov et al, 2013) CBOW 방식 w(t-2) ~ w(t+2)바탕으로 w(t)를 유추할 수 있도록 한다. 주변 단어들을 이용해서 중심단어를 학습한다. w(t-2) : The w(t-1) : quick w(t) : brown -&gt; output w(t+1) : fox w(t+2) : jumps w(t-2)가 들어갔을 때 brown이 나올 수 있도록 학습하는 것. 중심단어를 이용해서 주변 단어를 학습한다. 네모를 window라고 하고 중심단어로부터 주변단어까지의 사이즈를 window size라고한다 Skip-gram 방식 w(t)를 바탕으로 w(t-2) ~ w(t+2)를 유추할 수 있도록 한다. Objective Function : Learn to reflect similarity between two vectors The의 경우 빈도수가 높아서 과적합이 일어날 수 있다. -&gt; Subsampling to get : Regularize 전체 단어에 대한 대상으로 softmax를 계산하는 것이 아닌 window size 안에 있는 단어와 밖에 있는 단어는 샘플링하여 해당되는 내용을 바탕으로 softmax 계산을 한다. -&gt; Negative Sampling to get Speed Up Glove Global Vectors for Word Representation (Jeffrey Pennington et al, 2014)co-occurrence 개념을 제시window 밖에 있는 단어는 학습이 진행되지 않는 단점을 지적했다. Objective Function : Learn to reflect co-occurrence between two vectors가정 : 한 문장안에 동시 출연된 단어는 연관성이 있을 것이다. 연관성이 높으면 값이 높을 것이다 FastText Bag of Tricks for Efficient Text Classification (Armand Joulin et al, 2016) Objective Function : Learn to reflect similarity between two vectors코퍼스를 형성하는 단어를 적게두자.UnigramsBigramsTrigrams4-grams","link":"/2020/04/25/NLP/VectorforNLP/"},{"title":"Recurrent Neural Network","text":"Deep Learning Neural Network with Many Hidden Layers Learning through Back-Propagation from Objective Function (Loss Function) 인간의 뇌와 흡사 Type of RNN 빨간색은 input초록색은 hidden state파란색은 output 해석하는 형태도 존재이어져서 뭔가를 한다고 이해하자 Vanilla RNN xt와 ht-1을 같이 받는다Wh는 가중치행렬 Need to Refine Long Term Dependency문장길이가 토큰기준 3개정도인데만약 길이가 길게 되면 최종적 output값에 대해 발생하는 그레디언트 값이 앞쪽까지 전달되는데 문제가 있다. LSTM (Long Short Term Memory) 12개의 term이 학습이 된다Cell state가 있다xt는 현재시점t-1은 이전시점ft는 현재시점에 대한 input과 이전시점에대한 ht-1에 각각 가중치행렬을 곱해주고 b를 더함 이것을 시그모드취해준다.tanh는 -1 ~ 1 까지임 -&gt; 현재 위치에서 어느정도 정보를 반영할지에 대한 결과 forget gate : 현재시점 입력값과 이전시점의 hidden state의 결과에 시그모이드를 취한 값을 이용해서 몇퍼센트 기억할건지 하는 역할 input gate : 현재시점 입력값과 이전시점의 hidden state를 tanh를 해서 원소 곱 그리고 forget에서 나온 값과 더한다 -&gt; cell state 업데이트를 함 output gate : 업데이트한 cell state를 정보를 바탕으로 output을 통과한 결과에 cell state를 tanh를 해주고 원소 곱 -&gt; ht 와 yt로 내보내줌 GRU(Gated recurrent unit) lstm의 많은 학습을 9개로 줄인 것","link":"/2020/04/25/NLP/RecurrentNeuralNetwork/"},{"title":"시스템보안설계(1) - 서비스공격유형","text":"시스템 보안 설계서비스 공격 유형 서비스 거부 공격의 개념과 서비스 거부 공격의 종류별 특징을 이해 서버 인증과 서버 접근 통제의 개념을 이해하고 인증의 종류와 접근 통제 정책의 종류를 이해 보안아키텍처와 보안 Framework의 개념 및 보안 통제 항목의 종류를 이해 서비스 거부 공격 서비스 공격은 일반적으로 서비스 거부(Dos) 공격을 뜻함 서버는 서비스 요청에 대해 서비스를 제공하는 의무가 있음 서버가 서비스 능력을 초과하는 요청을 도시다발적으로 받게 되면 서비스 불능상태로 마비됨 Ping Flood 하나의 사이트로 많은 양의 ICMP echo request를 요청할 때 이 사이트에서 존재하는 시스템 자원은 개별 메시지들에 각각 응답하기 위해 시스템 자원(resource)를 모두 사용해 버리는 점을 이용 ICMP : Internet Control Message Protocol TCP/IP 기반의 인터넷 통신 서비스에서 인터넷 프로토콜에 결합되어 전송되는 프로토콜로, IP에 대해 통신 중 발생하는 오류 처리와 전송 경로 변경, 에코 요청, 에코 응답 등을 제어하기 위한 메시지를 취급1ping [주소] Ping of Death Ping 명령을 전송할 때 패킷의 크기를 인터넷 프로토콜 허용 범위(65,536 byte)이상으로 전송하여 공격 대상의 네트워크를 마비시키는 서비스 거부 공격 Ping 기본 크기 : 32byte1ping -I 100 -n 5 [주소] SMURFING ++엄청난 양의 데이터를 한 사이트에 집중적으로 보냄++으로써 네트워크를 불능 상태로 만듬 공격자는 ++패킷의 송신 주소를 공격 대상의 IP주소로 위장++하고 해당 네트워크 라우터의++ 브로드캐스트 주소를 수신지++로 하여 패킷을 전송하면 라우터의 브로드캐스트 주소로 ++수신된 패++킷은 해당 네트워크 내의 모든 컴퓨터로 전송함 Broadcast : 네트워크에 연결된 전체 컴퓨터에 패킷을 전송할 때 사용하는 주소 해당 네트워크 내의 모든 컴퓨터는 ++수신된 패킷에 대한 응답 메시지를 송신 주소인 공격 대상++ 컴퓨터로 ++집중적으로 전송++하게 되어 공격 대상지는 ++네트워크 과부하++로 정상적인 서비스 수행이 불가능해짐 SYN Flooding(Synchronize sequence number) TCP는 신뢰성 있는 전송을 위해 3-way-handshake를 거친 후에 데이터를 전송 Client &gt; ServerSYN 패킷 전송대상 서버에 접속한다고 손을 내밈 Server &gt; ClientSYN + ACK 패킷 전송서버에는 접속해도 된다고 응답함 Client &gt; ServerACK(Acknowledgment) 패킷 전송응답메시지를 받으면 데이터를 보냄 공격자가 ++가상의 클라이언트로 위장++한 후 공격 대상지인 서버로 ++’SYN’ 신호를 보냄++ 서버는 ++’SYN+ACK’신호를 가상의 클라이언트++로 보내면서 클라이언트의 ++접속을 받아들이기 위한 메모리의 일정 공간++을 확보함 공격자가 사용할 수 없는 IP 주소를 이용하여 공격 대상지 서버로 반복적인 3-way-handshake 과정을 요청하면 ++공격 대상지 서버는 메모리 공간을 점점 더 많이 확보한 상태에서 대기++하게 됨 대비SYN 수신 대기 시간을 줄이거나 침입 차단 시스템을 활용 TearDrop 데이터의 송수신과정에서 ++패킷++의 크기가 커서 여러 개로 분할되어 전송될 때 ++분할 순서++를 알 수 있도록 ++Fragment Offset 값++을 함께 전송 이때 ++Offset 값을 변경시켜 수신 측에서 패킷을 재조립할 때 오류로 인한 과부하++ 발생시켜 시스템 다운 유도 일반적으로 128kb로 토막낸다. 8개 다시 받을 때 합친다. 대비 Fragment Offset 잘못된 경우 해당 패킷을 폐기Land 패킷을 전송할 때 ++송신 IP주소와 수신 IP주소를 모두 공격 대상의 IP주소로 설정한 후 공격 대상에게 전송++하는 것 이 패킷을 받은 공격 대상은 송신 IP주소가 자신이므로 자신에게 응답을 수행하는데 이러한 패킷이 계속해서 전송될 경우 자신에 대해 ++무한 응++답하게 함 대비 송신 IP주소와 수신 IP 주소를 검사 DDoS(Distributed Denial of Service, 분산 서비스 거부) 공격 여러 곳에 ++분산된 공격 지점에서 한 곳에 서버에 대해 서비스 거부 공격++을 수행 일부 호스트에 다수의 에이전트를 관리할 수 있는 핸들러 프로그램을 설치하여 마스터로 지정후 공격 취약점이 있는 호스트들을 탐색한 후 이들에 분산 서비스 공격용 툴을 설치하여 좀비PC로 만듬 분산 서비스 공격용 툴 Trin00 TFN TFN2K Stacheldraht 네트워크 침해 공격 용어Smishing 스미싱 ++문자메시지++를 보낸 후 메시지에 있는 인터넷 주소를 클릭하면 ++악성코드를 설치하여 금융정보 빼냄++ APT(Advanced Persistent Threats, 지능형 지속 위협) 특정기업이나 ++조직 네트워크에 침투++해 활동 거점을 마련한 뒤 때를 기다리면서 ++보안을 무력화하고 정보를 수집한 다음 외부로 빼돌리는 형태++의 공격 내부자에게 악성코드가 포함된 이메일을 오랜 기간 동안 꾸준히 발송해 클릭하기를 기다림 스턱스넷과 같이 악성코드가 담긴 USB로 전파 Spear Phishing 스피어 피싱 ++사회 공학++의 한 기법 Social Engineering : ++인간 상호 작용++의 깊은 신뢰를 바탕으로 사람들을 속여 정상 보안절차를 깨뜨리기 위한 비기술적 시스템 침입 수단 특정 대상을 선정 후 ++일반적인 메일로 위장++ 후 지속적으로 발송하여 ++발송 메일의 링크나 첨부 파일을 클릭하도록 유도++해 사용자의 개인 정보를 탈취 Qshing 큐싱 ++QR(Quick Rsponse Code)를 통해 악성 앱의 다운로드 유도++ 정보 보안 침해 공격 용어Botnet 봇넷 ++악성 프로그램에 감염++되어 악의적인 의도로 사용될 수 있는 ++다수의 컴퓨터들이 네트워크로 연결++된 형태 Worm 웜 네트워크를 통해 ++연속적으로 자신을 복제++하여 시++스템의 부하를 높임++으로써 결국 시스템을 다운시키는 바이러스 Zero Day Attack 제로데이공격 ++보안 취약점이 발견++되었을 때 발견된 취약점의 존재 자체가 널리 공표되기 전에 해당 취약점을 통하여 이루어지는 보안 공격으로 공격의 신속성 의미 Ransomware 랜섬웨어 인터넷 사용자의 컴퓨터에 잠입해 ++내부 문서나 파일 등을 암호화++해 사용자가 열지 못하게 하는 프로그램 암호 해독용 프로그램의 전달을 조건으로 돈을 요구 Trojan Horse 트로이 목마 정상적인 기능을 하는 프로그램으로 위장 프로그램 내에 ++숨어 있다가 해당 프로그램이 동작할 때 활성화++ 되어 부작용을 일으킴 ++자기 복제 능력은 없음++ Key Logger Attack 키로거 공격 컴퓨터 ++사용자의 키보드 움직임을 탐지++해 id, pw, 계좌번호 등 같은 개인의 중요한 정보를 몰래 빼가는 해킹","link":"/2020/04/26/%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%82%AC/ch21/%EC%8B%9C%EC%8A%A4%ED%85%9C%EB%B3%B4%EC%95%88%EC%84%A4%EA%B3%84_01_%EC%84%9C%EB%B9%84%EC%8A%A4%EA%B3%B5%EA%B2%A9%EC%9C%A0%ED%98%95/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"class","slug":"class","link":"/tags/class/"},{"name":"논문","slug":"논문","link":"/tags/%EB%85%BC%EB%AC%B8/"},{"name":"한국어임베딩","slug":"한국어임베딩","link":"/tags/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/"},{"name":"embedding","slug":"embedding","link":"/tags/embedding/"},{"name":"자연어처리","slug":"자연어처리","link":"/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"Anaconda","slug":"Anaconda","link":"/tags/Anaconda/"},{"name":"데이터분석","slug":"데이터분석","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/"},{"name":"데이터사이언스개념","slug":"데이터사이언스개념","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4%EA%B0%9C%EB%85%90/"},{"name":"소프트스킬","slug":"소프트스킬","link":"/tags/%EC%86%8C%ED%94%84%ED%8A%B8%EC%8A%A4%ED%82%AC/"},{"name":"데이터분석 - 순열 - 조합","slug":"데이터분석-순열-조합","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%88%9C%EC%97%B4-%EC%A1%B0%ED%95%A9/"},{"name":"데이터분석 - 데이터사이언스개념 - python 자료형 - c 자료형","slug":"데이터분석-데이터사이언스개념-python-자료형-c-자료형","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4%EA%B0%9C%EB%85%90-python-%EC%9E%90%EB%A3%8C%ED%98%95-c-%EC%9E%90%EB%A3%8C%ED%98%95/"},{"name":"NLP배경","slug":"NLP배경","link":"/tags/NLP%EB%B0%B0%EA%B2%BD/"},{"name":"정보처리기사","slug":"정보처리기사","link":"/tags/%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%82%AC/"},{"name":"서비스공격유형","slug":"서비스공격유형","link":"/tags/%EC%84%9C%EB%B9%84%EC%8A%A4%EA%B3%B5%EA%B2%A9%EC%9C%A0%ED%98%95/"}],"categories":[{"name":"파이썬","slug":"파이썬","link":"/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/"},{"name":"논문","slug":"논문","link":"/categories/%EB%85%BC%EB%AC%B8/"},{"name":"자연어처리","slug":"자연어처리","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"문법","slug":"파이썬/문법","link":"/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EB%AC%B8%EB%B2%95/"},{"name":"임베딩","slug":"자연어처리/임베딩","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/%EC%9E%84%EB%B2%A0%EB%94%A9/"},{"name":"Anaconda","slug":"자연어처리/Anaconda","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/Anaconda/"},{"name":"데이터사이언스","slug":"데이터사이언스","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/"},{"name":"데이터분석","slug":"데이터사이언스/데이터분석","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/"},{"name":"독서","slug":"독서","link":"/categories/%EB%8F%85%EC%84%9C/"},{"name":"NLP","slug":"자연어처리/NLP","link":"/categories/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/NLP/"},{"name":"정보처리기사","slug":"정보처리기사","link":"/categories/%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%82%AC/"},{"name":"시스템보안설계","slug":"정보처리기사/시스템보안설계","link":"/categories/%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%82%AC/%EC%8B%9C%EC%8A%A4%ED%85%9C%EB%B3%B4%EC%95%88%EC%84%A4%EA%B3%84/"}]}